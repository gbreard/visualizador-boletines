{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando hoja vertical: C1.1...\n",
      "Error al leer metadatos de C1.1: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "Error procesando hoja vertical C1.1: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "  ✓ Hoja C1.1 procesada: 0 filas\n",
      "Procesando hoja vertical: C1.2...\n",
      "Error al leer metadatos de C1.2: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "Error procesando hoja vertical C1.2: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "  ✓ Hoja C1.2 procesada: 0 filas\n",
      "Procesando hoja vertical: C2.1...\n",
      "Error al leer metadatos de C2.1: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "Error procesando hoja vertical C2.1: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "  ✓ Hoja C2.1 procesada: 0 filas\n",
      "Procesando hoja vertical: C2.2...\n",
      "Error al leer metadatos de C2.2: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "Error procesando hoja vertical C2.2: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "  ✓ Hoja C2.2 procesada: 0 filas\n",
      "Procesando hoja horizontal: C3...\n",
      "Error al leer metadatos de C3: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "Error procesando hoja horizontal C3: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "  ✓ Hoja C3 procesada: 0 filas\n",
      "Procesando hoja horizontal: C4...\n",
      "Error al leer metadatos de C4: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "Error procesando hoja horizontal C4: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "  ✓ Hoja C4 procesada: 0 filas\n",
      "Procesando hoja horizontal: C5...\n",
      "Error al leer metadatos de C5: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "Error procesando hoja horizontal C5: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "  ✓ Hoja C5 procesada: 0 filas\n",
      "Procesando hoja horizontal: C6...\n",
      "Error al leer metadatos de C6: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "Error procesando hoja horizontal C6: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "  ✓ Hoja C6 procesada: 0 filas\n",
      "Procesando hoja horizontal: C7...\n",
      "Error al leer metadatos de C7: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "Error procesando hoja horizontal C7: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "  ✓ Hoja C7 procesada: 0 filas\n",
      "Procesando descriptores de actividad...\n",
      "Error generando archivo de descriptores: [Errno 2] No such file or directory: 'Data\\\\nacional_serie_empleo_trimestral_actualizado241312.xlsx'\n",
      "Archivo no encontrado: Data/descriptores_actividad_raw.txt\n",
      "  ✓ Descriptores procesados: 0 registros\n",
      "Concatenando y limpiando DataFrames...\n",
      "  ! No hay datos verticales para concatenar\n",
      "  ! No hay datos horizontales para concatenar\n",
      "Guardando archivos CSV...\n",
      "  ✓ Guardado: Data/preprocessed_vertical_clean.csv\n",
      "  ✓ Guardado: Data/preprocessed_horizontal_clean.csv\n",
      "  ! No se guardó Data/descriptores_actividad.csv (sin datos)\n",
      "  ✓ Guardado: Data/metadata_hojas.csv\n",
      "\n",
      "¡Preprocesamiento completado con éxito!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1) Funciones para homogeneizar y limpiar los datos\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def unify_period_string(period_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Unifica variaciones del texto del período a un formato estándar.\n",
    "    Ejemplos:\n",
    "      - \"1er Trim 1996\" -> \"1º Trim 1996\"\n",
    "      - \"2do Trim 2020\" -> \"2º Trim 2020\"\n",
    "    \"\"\"\n",
    "    if pd.isna(period_str):\n",
    "        return \"\"\n",
    "    \n",
    "    p = str(period_str).strip().lower()\n",
    "    # Reemplazos comunes para números ordinales\n",
    "    p = p.replace(\"1er\", \"1º\")\n",
    "    p = p.replace(\"2do\", \"2º\")\n",
    "    p = p.replace(\"3er\", \"3º\")\n",
    "    p = p.replace(\"4to\", \"4º\")\n",
    "    # Uniformar \"trim\"\n",
    "    p = p.replace(\"trim\", \"Trim\")\n",
    "    # Eliminar espacios dobles\n",
    "    p = re.sub(r\"\\s+\", \" \", p)\n",
    "    # Convertir a \"title\" para tener \"Trim\" con mayúscula y dejar el formato uniforme\n",
    "    p = p.title()\n",
    "    return p\n",
    "\n",
    "def clean_footnotes(df: pd.DataFrame, periodo_col=\"Período\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Limpia el DataFrame:\n",
    "      - Unifica el formato del Período.\n",
    "      - Descarta filas cuyo valor en Período no sea exactamente un trimestre válido.\n",
    "      - Si existe la columna \"Valor\", se descartan filas en las que ese valor no sea numérico.\n",
    "      - Filtra filas que contengan palabras clave típicas de notas.\n",
    "      \n",
    "    Se espera que un período válido tenga el formato:\n",
    "       \"1º Trim YYYY\" (con 1-4 para el trimestre, y un año de 4 dígitos).\n",
    "    \"\"\"\n",
    "    if periodo_col not in df.columns:\n",
    "        return df\n",
    "\n",
    "    # 1) Unificar el texto en la columna de período\n",
    "    df[periodo_col] = df[periodo_col].astype(str).apply(unify_period_string)\n",
    "    \n",
    "    # 2) Expresión regular para detectar períodos válidos, por ejemplo \"1º Trim 1996\"\n",
    "    pattern_periodo = re.compile(r\"^[1-4]º Trim \\d{4}$\", re.IGNORECASE)\n",
    "    mask_periodo_valido = df[periodo_col].str.match(pattern_periodo)\n",
    "    \n",
    "    # 3) Descartar filas que tengan palabras clave de notas\n",
    "    keywords = [\"nota\", \"fuente\", \"variación\", \"observación\", \n",
    "                \"comentario\", \"disclaimer\", \"empresas\", \"desocup\"]\n",
    "    pattern_keywords = re.compile(\"|\".join(keywords), re.IGNORECASE)\n",
    "    mask_no_keywords = ~df[periodo_col].str.contains(pattern_keywords, na=False)\n",
    "    \n",
    "    # 4) Si existe la columna \"Valor\", verificar que sea numérica\n",
    "    if \"Valor\" in df.columns:\n",
    "        mask_valor_numeric = pd.to_numeric(df[\"Valor\"], errors=\"coerce\").notnull()\n",
    "    else:\n",
    "        mask_valor_numeric = True\n",
    "    \n",
    "    # 5) Combinar todas las máscaras\n",
    "    mask_total = mask_periodo_valido & mask_no_keywords & mask_valor_numeric\n",
    "    \n",
    "    df_limpio = df[mask_total].copy()\n",
    "    return df_limpio\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2) Funciones para extraer metadatos y procesar hojas\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def extract_metadata(file_path: str, sheet_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Lee las dos primeras filas de la hoja (row 0 y row 1) sin encabezado (header=None)\n",
    "    y devuelve un diccionario con el título y subtítulo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_header = pd.read_excel(file_path, sheet_name=sheet_name, header=None, nrows=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer metadatos de {sheet_name}: {e}\")\n",
    "        return {\"Sheet\": sheet_name, \"Titulo\": \"\", \"Subtitulo\": \"\"}\n",
    "    \n",
    "    titulo = \"\"\n",
    "    subtitulo = \"\"\n",
    "    if len(df_header) > 0 and len(df_header.columns) > 0:\n",
    "        val_titulo = df_header.iloc[0, 0]\n",
    "        titulo = str(val_titulo) if pd.notna(val_titulo) else \"\"\n",
    "    if len(df_header) > 1 and len(df_header.columns) > 0:\n",
    "        val_subtitulo = df_header.iloc[1, 0]\n",
    "        subtitulo = str(val_subtitulo) if pd.notna(val_subtitulo) else \"\"\n",
    "    \n",
    "    return {\"Sheet\": sheet_name, \"Titulo\": titulo.strip(), \"Subtitulo\": subtitulo.strip()}\n",
    "\n",
    "def process_vertical_sheet(file_path: str, sheet_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Procesa hojas verticales (donde el período está en la primera columna).\n",
    "    Se asume que la fila de encabezados es la 3ra (índice 2).\n",
    "    Optimizado para reducir uso de memoria.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Primero leemos una muestra para determinar columnas útiles\n",
    "        df_sample = pd.read_excel(file_path, sheet_name=sheet_name, header=2, nrows=5)\n",
    "        valid_cols = [col for col in df_sample.columns \n",
    "                     if not (isinstance(col, str) and \"Volver al índice\" in col)]\n",
    "        \n",
    "        # Leemos solo las columnas válidas\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name, header=2, usecols=valid_cols)\n",
    "        \n",
    "        # Convertimos tipos de datos para optimizar memoria\n",
    "        if \"Período\" in df.columns:\n",
    "            df[\"Período\"] = df[\"Período\"].astype(str).str.strip()\n",
    "        \n",
    "        df[\"Fuente\"] = sheet_name\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando hoja vertical {sheet_name}: {e}\")\n",
    "        # Devolver DataFrame vacío en caso de error\n",
    "        return pd.DataFrame(columns=[\"Período\", \"Valor\", \"Fuente\"])\n",
    "\n",
    "def process_horizontal_sheet(file_path: str, sheet_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Procesa hojas horizontales (donde el período está en los encabezados de columnas).\n",
    "    Se asume que la fila de encabezados es la 3ra (índice 2) y se transforma a formato long.\n",
    "    Implementa procesamiento por lotes para reducir uso de memoria.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Leer una muestra para determinar columnas útiles\n",
    "        df_sample = pd.read_excel(file_path, sheet_name=sheet_name, header=2, nrows=5)\n",
    "        valid_cols = [col for col in df_sample.columns \n",
    "                     if not (isinstance(col, str) and \"Volver al índice\" in col)]\n",
    "        \n",
    "        # Leer solo las columnas válidas\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name, header=2, usecols=valid_cols)\n",
    "        \n",
    "        first_col = df.columns[0]\n",
    "        if not isinstance(first_col, str) or first_col.strip() == \"\":\n",
    "            df = df.rename(columns={first_col: \"Categoría\"})\n",
    "        if \"Categoría\" not in df.columns:\n",
    "            df = df.reset_index().rename(columns={\"index\": \"Categoría\"})\n",
    "        \n",
    "        # Procesamiento por lotes para evitar MemoryError\n",
    "        result_chunks = []\n",
    "        chunk_size = 100  # Ajustar según memoria disponible\n",
    "        \n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            try:\n",
    "                chunk = df.iloc[i:i+chunk_size].copy()\n",
    "                chunk_long = chunk.melt(id_vars=[\"Categoría\"], var_name=\"Período\", value_name=\"Valor\")\n",
    "                chunk_long[\"Fuente\"] = sheet_name\n",
    "                result_chunks.append(chunk_long)\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando lote {i} de {sheet_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if result_chunks:\n",
    "            return pd.concat(result_chunks, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame(columns=[\"Categoría\", \"Período\", \"Valor\", \"Fuente\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando hoja horizontal {sheet_name}: {e}\")\n",
    "        return pd.DataFrame(columns=[\"Categoría\", \"Período\", \"Valor\", \"Fuente\"])\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.1) Función para procesar 'descriptores_actividad' a partir del Excel\n",
    "# y guardarlo como archivo de texto, para luego procesarlo línea por línea.\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def generate_descriptores_txt_from_excel(file_path: str, sheet_name: str, output_txt: str):\n",
    "    \"\"\"\n",
    "    Lee la hoja 'Descriptores de actividad' del Excel y guarda su contenido\n",
    "    en un archivo de texto (una línea por registro), usando la primera columna.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_desc = pd.read_excel(file_path, sheet_name=sheet_name, header=None, usecols=[0])\n",
    "        \n",
    "        # Asegurarse de que el directorio exista\n",
    "        os.makedirs(os.path.dirname(output_txt), exist_ok=True)\n",
    "        \n",
    "        # Escribir cada línea en el archivo\n",
    "        with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "            for index, row in df_desc.iterrows():\n",
    "                if pd.notna(row[0]):\n",
    "                    f.write(str(row[0]) + \"\\n\")\n",
    "        \n",
    "        print(f\"Archivo de descriptores generado: {output_txt}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generando archivo de descriptores: {e}\")\n",
    "\n",
    "def process_descriptores_txt(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee el archivo de texto generado con descriptores línea por línea,\n",
    "    descarta encabezados y extrae el código y la descripción.\n",
    "    Retorna un DataFrame con columnas ['Codigo', 'DescripcionCompleta'].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Archivo no encontrado: {file_path}\")\n",
    "            return pd.DataFrame(columns=[\"Codigo\", \"DescripcionCompleta\"])\n",
    "        \n",
    "        lines_limpias = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if \"Descriptor completo de actividad\" in line:\n",
    "                    continue\n",
    "                \n",
    "                # Intentar extraer código y descripción separando por la primera coma\n",
    "                match = re.match(r\"^([^,]+),\\s*(.*)$\", line)\n",
    "                if match:\n",
    "                    codigo = match.group(1).strip()\n",
    "                    descripcion = match.group(2).strip()\n",
    "                    lines_limpias.append((codigo, descripcion))\n",
    "        \n",
    "        if lines_limpias:\n",
    "            df = pd.DataFrame(lines_limpias, columns=[\"Codigo\", \"DescripcionCompleta\"])\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame(columns=[\"Codigo\", \"DescripcionCompleta\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando archivo de descriptores: {e}\")\n",
    "        return pd.DataFrame(columns=[\"Codigo\", \"DescripcionCompleta\"])\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3) Script principal de preprocesamiento\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def main_preprocessing_with_metadata_and_cleanfootnotes():\n",
    "    # Configuración para reducir uso de memoria\n",
    "    pd.options.mode.chained_assignment = None  # Evitar warnings de copia\n",
    "    \n",
    "    # Ruta al archivo Excel\n",
    "    file_path = r\"Data\\nacional_serie_empleo_trimestral_actualizado241312.xlsx\"\n",
    "    \n",
    "    # Asegurarse de que el directorio Data exista\n",
    "    os.makedirs(\"Data\", exist_ok=True)\n",
    "    \n",
    "    # Definir hojas verticales y horizontales\n",
    "    vertical_sheets = [\"C1.1\", \"C1.2\", \"C2.1\", \"C2.2\"]\n",
    "    horizontal_sheets = [\"C3\", \"C4\", \"C5\", \"C6\", \"C7\"]\n",
    "    \n",
    "    dfs = {}\n",
    "    metadata_list = []\n",
    "    \n",
    "    # Procesar hojas verticales\n",
    "    for sheet in vertical_sheets:\n",
    "        try:\n",
    "            print(f\"Procesando hoja vertical: {sheet}...\")\n",
    "            meta = extract_metadata(file_path, sheet)\n",
    "            metadata_list.append(meta)\n",
    "            df_v = process_vertical_sheet(file_path, sheet)\n",
    "            dfs[sheet] = df_v\n",
    "            print(f\"  ✓ Hoja {sheet} procesada: {len(df_v)} filas\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error procesando hoja {sheet}: {e}\")\n",
    "            dfs[sheet] = pd.DataFrame()  # DataFrame vacío en caso de error\n",
    "    \n",
    "    # Procesar hojas horizontales\n",
    "    for sheet in horizontal_sheets:\n",
    "        try:\n",
    "            print(f\"Procesando hoja horizontal: {sheet}...\")\n",
    "            meta = extract_metadata(file_path, sheet)\n",
    "            metadata_list.append(meta)\n",
    "            df_h = process_horizontal_sheet(file_path, sheet)\n",
    "            dfs[sheet] = df_h\n",
    "            print(f\"  ✓ Hoja {sheet} procesada: {len(df_h)} filas\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error procesando hoja {sheet}: {e}\")\n",
    "            dfs[sheet] = pd.DataFrame()  # DataFrame vacío en caso de error\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # 3.1) Procesar hoja de Descriptores de actividad\n",
    "    # --------------------------------------------------------------------------\n",
    "    descriptores_sheet = \"Descriptores de actividad\"\n",
    "    descriptores_txt = \"Data/descriptores_actividad_raw.txt\"\n",
    "    try:\n",
    "        print(\"Procesando descriptores de actividad...\")\n",
    "        # Generar el archivo de texto desde el Excel\n",
    "        generate_descriptores_txt_from_excel(file_path, descriptores_sheet, descriptores_txt)\n",
    "        # Procesar el archivo de texto para obtener el DataFrame\n",
    "        df_descriptores = process_descriptores_txt(descriptores_txt)\n",
    "        print(f\"  ✓ Descriptores procesados: {len(df_descriptores)} registros\")\n",
    "        dfs[\"Descriptores de actividad\"] = df_descriptores\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error procesando descriptores: {e}\")\n",
    "        df_descriptores = pd.DataFrame(columns=[\"Codigo\", \"DescripcionCompleta\"])\n",
    "        dfs[\"Descriptores de actividad\"] = df_descriptores\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # 3.2) Concatenar DataFrames verticales y horizontales\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"Concatenando y limpiando DataFrames...\")\n",
    "    \n",
    "    # Filtrar DataFrames vacíos antes de concatenar\n",
    "    vertical_dfs = [dfs[sheet] for sheet in vertical_sheets if sheet in dfs and not dfs[sheet].empty]\n",
    "    horizontal_dfs = [dfs[sheet] for sheet in horizontal_sheets if sheet in dfs and not dfs[sheet].empty]\n",
    "    \n",
    "    if vertical_dfs:\n",
    "        df_vertical = pd.concat(vertical_dfs, ignore_index=True)\n",
    "        print(f\"  ✓ DataFrame vertical concatenado: {len(df_vertical)} filas\")\n",
    "    else:\n",
    "        df_vertical = pd.DataFrame(columns=[\"Período\", \"Valor\", \"Fuente\"])\n",
    "        print(\"  ! No hay datos verticales para concatenar\")\n",
    "    \n",
    "    if horizontal_dfs:\n",
    "        df_horizontal = pd.concat(horizontal_dfs, ignore_index=True)\n",
    "        print(f\"  ✓ DataFrame horizontal concatenado: {len(df_horizontal)} filas\")\n",
    "    else:\n",
    "        df_horizontal = pd.DataFrame(columns=[\"Categoría\", \"Período\", \"Valor\", \"Fuente\"])\n",
    "        print(\"  ! No hay datos horizontales para concatenar\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # 4) Limpiar y homogeneizar la columna de Período en ambos dataframes\n",
    "    # --------------------------------------------------------------------------\n",
    "    try:\n",
    "        if not df_vertical.empty:\n",
    "            df_vertical_clean = clean_footnotes(df_vertical, \"Período\")\n",
    "            print(f\"  ✓ DataFrame vertical limpiado: {len(df_vertical_clean)} filas\")\n",
    "        else:\n",
    "            df_vertical_clean = df_vertical\n",
    "        \n",
    "        if not df_horizontal.empty:\n",
    "            df_horizontal_clean = clean_footnotes(df_horizontal, \"Período\")\n",
    "            print(f\"  ✓ DataFrame horizontal limpiado: {len(df_horizontal_clean)} filas\")\n",
    "        else:\n",
    "            df_horizontal_clean = df_horizontal\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error en limpieza de datos: {e}\")\n",
    "        df_vertical_clean = df_vertical\n",
    "        df_horizontal_clean = df_horizontal\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # 5) Merge df_horizontal_clean con df_descriptores para incluir la descripción\n",
    "    # --------------------------------------------------------------------------\n",
    "    try:\n",
    "        if not df_horizontal_clean.empty and not df_descriptores.empty:\n",
    "            df_horizontal_clean = pd.merge(\n",
    "                df_horizontal_clean,\n",
    "                df_descriptores,\n",
    "                left_on=\"Categoría\",\n",
    "                right_on=\"Codigo\",\n",
    "                how=\"left\"\n",
    "            )\n",
    "            print(f\"  ✓ Merge con descriptores completado\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error en merge con descriptores: {e}\")\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # 6) Guardar los resultados en archivos CSV dentro de Data\n",
    "    # --------------------------------------------------------------------------\n",
    "    try:\n",
    "        output_vertical = \"Data/preprocessed_vertical_clean.csv\"\n",
    "        output_horizontal = \"Data/preprocessed_horizontal_clean.csv\"\n",
    "        output_descriptores = \"Data/descriptores_actividad.csv\"\n",
    "        output_metadata = \"Data/metadata_hojas.csv\"\n",
    "        \n",
    "        # Guardar con manejo de errores\n",
    "        print(\"Guardando archivos CSV...\")\n",
    "        df_vertical_clean.to_csv(output_vertical, index=False)\n",
    "        print(f\"  ✓ Guardado: {output_vertical}\")\n",
    "        \n",
    "        df_horizontal_clean.to_csv(output_horizontal, index=False)\n",
    "        print(f\"  ✓ Guardado: {output_horizontal}\")\n",
    "        \n",
    "        if not df_descriptores.empty:\n",
    "            df_descriptores.to_csv(output_descriptores, index=False)\n",
    "            print(f\"  ✓ Guardado: {output_descriptores}\")\n",
    "        else:\n",
    "            print(f\"  ! No se guardó {output_descriptores} (sin datos)\")\n",
    "        \n",
    "        df_metadata = pd.DataFrame(metadata_list)\n",
    "        df_metadata.to_csv(output_metadata, index=False)\n",
    "        print(f\"  ✓ Guardado: {output_metadata}\")\n",
    "        \n",
    "        print(\"\\n¡Preprocesamiento completado con éxito!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error guardando archivos: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main_preprocessing_with_metadata_and_cleanfootnotes()\n",
    "    except Exception as e:\n",
    "        print(f\"Error en el script principal: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
